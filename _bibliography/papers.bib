---
---
@string{aps = {American Physical Society,}}


@article{M2hub,
title = {M²Hub: Unlocking the Potential of Machine Learning for Materials Discovery},
author = {Du*, Yuanqi and Wang*, Yingheng and Huang, Yining and Li, Jianan Canal and Zhu, Yanqiao and Xie, Tian and Duan, Chenru and Gregoire, John M. and Gomes, Carla P.},
journal = {NeurIPS 2023 Datasets and Benchmarks Track,},
year = {2023},
preview={m2lab.png},
code={https://github.com/yuanqidu/M2Hub},
pdf={https://arxiv.org/abs/2307.05378},
abstract = {M2Hub aims to build the machine learning foundations for materials discovery which has a standard workflow from virtual screening/inverse design to simulation to experiment. M2Hub provides data downloading, data processing, (baseline and state-of-the-art) machine learning method implementation, evaluation pipeline and benchmark results.}
}

@article{clean,
title = {Enzyme Function Prediction using Contrastive Learning},
author = {Yu*, Tianhao and Cui*, Haiyang and Li, Jianan Canal and Luo, Yunan and Zhao, Huimin},
journal = {Science,},
volume = {379},
number = {6639},
pages = {1358-1363},
year = {2023},
preview={CLEAN.png},
doi = {10.1126/science.adf2465},
poster={CLEAN_Poster.pdf},
code={https://github.com/tttianhao/CLEAN},
pdf={https://www.science.org/doi/10.1126/science.adf2465},
abstract = {Enzyme function annotation is a fundamental challenge, and numerous computational tools have been developed. However, most of these tools cannot accurately predict functional annotations, such as enzyme commission (EC) number, for less-studied proteins or those with previously uncharacterized functions or multiple activities. We present a machine learning algorithm named CLEAN (contrastive learning–enabled enzyme annotation) to assign EC numbers to enzymes with better accuracy, reliability, and sensitivity compared with the state-of-the-art tool BLASTp. The contrastive learning framework empowers CLEAN to confidently (i) annotate understudied enzymes, (ii) correct mislabeled enzymes, and (iii) identify promiscuous enzymes with two or more EC numbers—functions that we demonstrate by systematic in silico and in vitro experiments. We anticipate that this tool will be widely used for predicting the functions of uncharacterized enzymes, thereby advancing many fields, such as genomics, synthetic biology, and biocatalysis.}
}

@article{gLM,
title={Genomic Language Models: Opportunities and Challenges}, 
author={Benegas*, Gonzalo and Ye*, Chengzhong and Albors*, Carlos and Li*, Jianan Canal and Song, Yun S.},
year={2024},
eprint={2407.11435},
archivePrefix={arXiv},
preview={gLM.png},
primaryClass={q-bio.GN},
pdf={https://arxiv.org/abs/2407.11435}, 
abstract = {Large language models (LLMs) are having transformative impacts across a wide range of scientific fields, particularly in the biomedical sciences. Just as the goal of Natural Language Processing is to understand sequences of words, a major objective in biology is to understand biological sequences. Genomic Language Models (gLMs), which are LLMs trained on DNA sequences, have the potential to significantly advance our understanding of genomes and how DNA elements at various scales interact to give rise to complex functions. In this review, we showcase this potential by highlighting key applications of gLMs, including fitness prediction, sequence design, and transfer learning. Despite notable recent progress, however, developing effective and efficient gLMs presents numerous challenges, especially for species with large, complex genomes. We discuss major considerations for developing and evaluating gLMs.}
}



@article{mctensor,
title = {MCTensor: A High-Precision Deep Learning Library with Multi-Component Floating-Point},
author = {Yu*, Tao and Guo*, Wentao and Li*, Jianan Canal and Yuan*, Tiancheng and De Sa, Christopher},
journal={ICML Workshop on Hardware Aware Efficient Training (HAET),},
year = {2022},
preview={MCTensor.png},
doi = {10.48550/arXiv.2207.08867},
pdf={MCTensor.pdf},
poster={MCTensor_poster.pdf},
code={https://github.com/ydtydr/MCTensor},
abstract={In this paper, we introduce MCTensor, a library based on PyTorch for providing general-purpose and high-precision arithmetic for DL training. MCTensor is used in the same way as PyTorch Tensor: we implement multiple basic, matrix-level computation operators and NN modules for MCTensor with identical PyTorch interface. Our algorithms achieve high precision computation and also benefits from heavily-optimized PyTorch floating-point arithmetic. We evaluate MCTensor arithmetic against PyTorch native arithmetic for a series of tasks, where models using MCTensor in float16 would match or outperform the PyTorch model with float32 or float64 precision.},
}
 
@article{cKAM,
author = {Li*, Jianan Canal and Zeng*, Yimeng and Guo*, Wentao},
year = {2022},
preview={cKAM.png},
title = {Cyclical Kernel Adaptive Metropolis},
journal={ArXiv Preprint,},
doi = {10.48550/arXiv.2206.14421},
code={https://github.com/canallee/Cyclical-Kernel-Adaptive-Metropolis},
pdf={cKAM.pdf},
abstract={We propose cKAM, cyclical Kernel Adaptive Metropolis, which incorporates a cyclical stepsize scheme to allow control for exploration and sampling. We show that on a crafted bimodal distribution, existing Adaptive Metropolis type algorithms would fail to converge to the true posterior distribution. We point out that this is because adaptive samplers estimates the local/global covariance structure using past history of the chain, which will lead to adaptive algorithms be trapped in a local mode. We demonstrate that cKAM encourages exploration of the posterior distribution and allows the sampler to escape from a local mode, while maintaining the high performance of adaptive methods.}
}



 
